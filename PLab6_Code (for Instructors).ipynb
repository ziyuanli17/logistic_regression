{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "PLab6_Code (for Instructors).ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyPIe/HzMLpv7tQhj5BwpQ/r"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLbL-LlnOAgm",
    "colab_type": "text"
   },
   "source": [
    "# PLab 6 Sample Code for Instructors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0aSWrpaElDM0",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Add import statements here\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "from scipy import stats"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TDvZ5xhsl9XM",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# To access files in your Google Drive, run this block and follow the instructions\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hi_dRrJDmtiI",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# To test if the above block worked, run this block\n",
    "# !ls '/content/gdrive/My Drive/'"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1rrY8fIxz7h",
    "colab_type": "text"
   },
   "source": [
    " ## Find test error\n",
    "\n",
    "The `find_test_error` function computes the test error of a linear classifier $w$. \n",
    "\n",
    "The hypothesis is assumed to be of the form $sign([1, x(N,:)] \\cdot w)$.\n",
    "\n",
    "Inputs:\n",
    "* `w` is the weight vector\n",
    "* `X` is the data matrix (without an initial column of 1's)\n",
    "* `y` are the data labels (plus or minus 1)\n",
    "\n",
    "Outputs:\n",
    "* `test_error` is the binary error of $w$ on the data set $(X, y)$ error; this should be between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0BCKbvjMlHtE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def find_test_error(w, X, y):\n",
    "    # Find the sigmoid distribution\n",
    "    sig = np.exp(np.dot(X, w))/(1+np.exp(np.dot(X, w)))\n",
    "    # Determine binary classification result\n",
    "    C = 0.5\n",
    "    binary_class = []\n",
    "    for s in sig:\n",
    "        if s >= C:\n",
    "            binary_class.append(1)\n",
    "        else:\n",
    "            binary_class.append(-1)\n",
    "\n",
    "    # Determine binary classification error\n",
    "    error_sum = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != binary_class[i]:\n",
    "            error_sum += 1\n",
    "\n",
    "    test_error = error_sum/len(y)\n",
    "    return test_error"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUF6Mr1V0S5T",
    "colab_type": "text"
   },
   "source": [
    " ## Logistic Regression\n",
    "\n",
    "The `logistic_reg`  learn a logistic regression model using gradient descent.\n",
    "\n",
    "Inputs:\n",
    "* `X` is the data matrix (without an initial column of 1's)\n",
    "* `y` are the data labels (plus or minus 1)\n",
    "* `w_init` is the initial value of the w vector ($d+1$ dimensional)\n",
    "* `max_its` is the maximum number of iterations to run for\n",
    "* `eta` is the learning rate\n",
    "\n",
    "Outputs:\n",
    "* t is the number of iterations gradient descent ran for\n",
    "* w is the learned weight vector\n",
    "* e_in is the in-sample (cross-entropy) error "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dTcJkPE6lHvg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def logistic_reg(X, y, w_init, max_its, eta, thresh): # According to TA, “You could edit the function to take in that threshold value.” I can also use global var but this is bad in python\n",
    "\n",
    "    # Define parameters\n",
    "    N=len(X)\n",
    "    w=w_init\n",
    "    t=0\n",
    "    while t < max_its:\n",
    "        # Calculate gradient\n",
    "        g_t = []\n",
    "        for n in range(N):\n",
    "            g_t.append((y[n][0]*X[n])/(1+np.exp(y[n][0]*np.dot(w, X[n]))))\n",
    "\n",
    "        g_t = -np.mean(g_t, 0)\n",
    "\n",
    "        # Test termination\n",
    "        mag_g_t = abs(g_t)\n",
    "        if all(m < thresh for m in mag_g_t):\n",
    "            break\n",
    "\n",
    "        # Update weight\n",
    "        v_t = -g_t\n",
    "        w = w+eta*v_t\n",
    "        t+=1\n",
    "\n",
    "    # Calculate the cross-entropy in-sample error\n",
    "    e_in = []\n",
    "    for n in range(N):\n",
    "        e_in.append(np.log((1+np.exp(-y[n][0]*np.dot(w, X[n])))))\n",
    "    e_in = np.mean(e_in)\n",
    "    return t, w, e_in"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7d-boqb0y_H",
    "colab_type": "text"
   },
   "source": [
    "## Run and Plot\n",
    "\n",
    "Run your code and plot figures below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FWHPRXv4lHx6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Read files using csv library (function defined to prevent repeatedness)\n",
    "def read_csv(file_name):\n",
    "    set=[]\n",
    "    # Read file\n",
    "    with open(file_name, 'r') as f:\n",
    "        file = list(csv.reader(f))[1:]\n",
    "        # Convert values\n",
    "        for l in file:\n",
    "            row = []\n",
    "            for n in l:\n",
    "                try:\n",
    "                    row.append(int(n))\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        row.append(float(n))\n",
    "                    except ValueError:\n",
    "                        print(\"Unexpected value\")\n",
    "                        exit(1)\n",
    "            set.append(row)\n",
    "    return set\n",
    "\n",
    "# Split into X and y\n",
    "def split_X_y(set):\n",
    "    X = []\n",
    "    y = []\n",
    "    for r in set:\n",
    "        X.append([1]+r[0:-1])\n",
    "        y.append([r[-1]])\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == [0]:\n",
    "            y[i] = [-1]\n",
    "    return X, y\n",
    "\n",
    "# Read training and testing files\n",
    "train_set = read_csv(\"cleveland_train.csv\")\n",
    "test_set = read_csv(\"cleveland_test.csv\")\n",
    "\n",
    "# Get X and y for training\n",
    "X = np.array(split_X_y(train_set)[0])\n",
    "y = np.array(split_X_y(train_set)[1])\n",
    "\n",
    "# Get X and y for testing\n",
    "X_t = np.array(split_X_y(test_set)[0])\n",
    "y_t = np.array(split_X_y(test_set)[1])\n",
    "\n",
    "# Experiment with iterations\n",
    "# Define input parameters\n",
    "eta_0 = 0.00001\n",
    "w_init = np.zeros(len(X[0]))\n",
    "iterations = [10000, 100000, 1000000]\n",
    "print(\"Experimenting with iterations...\\n\")\n",
    "for iter in iterations:\n",
    "    # Start training\n",
    "    start = time.time()\n",
    "    t, w, e_in = logistic_reg(X, y, w_init, iter, eta_0, 0.001)\n",
    "    end = time.time()\n",
    "\n",
    "    # Start testing\n",
    "    test_error = find_test_error(w, X_t, y_t)\n",
    "    training_error = find_test_error(w, X, y)\n",
    "\n",
    "    # Print out the results\n",
    "    print('Number of iterations: {}, Training time : {}s, In-sample Cross-Entropy error (Ein): {}, Binary error on the training set (Etrain): {}, Binary error on the test set (Etest): {} \\n'.format(t,round(end - start, 5), round(e_in, 5), round(training_error, 5), round(test_error, 5)))\n",
    "\n",
    "\n",
    "\n",
    "# Experiment with learning rate\n",
    "# Define parameters\n",
    "eta_0s=[0.01, 0.1, 1, 4, 5, 6, 7, 7.5, 7.6, 7.65]\n",
    "w_init = np.zeros(len(X[0]))\n",
    "\n",
    "# Find z-scores\n",
    "ZX = np.append(np.ones((len(X), 1)), stats.zscore(X[:,1:]), axis=1)\n",
    "ZX_t = np.append(np.ones((len(X_t), 1)), stats.zscore(X_t[:,1:]), axis=1)\n",
    "\n",
    "print(\"Experimenting with learning rates...\\n\")\n",
    "for eta_0 in eta_0s:\n",
    "    # Start training\n",
    "    # Using iterative termination condition of inf is equivalent to no iterations-based termination criteria\n",
    "    # Only terminate when the magnitude of every element of the gradient is less than 10^−6\n",
    "    start = time.time()\n",
    "    t, w, e_in = logistic_reg(ZX, y, w_init, float(\"inf\"), eta_0, 0.000001)\n",
    "    end = time.time()\n",
    "\n",
    "    # Start testing\n",
    "    test_error = find_test_error(w, ZX_t, y_t)\n",
    "\n",
    "    # Print out the results\n",
    "    print('Learning rate (η0): {}, Number of iterations: {}, Training time : {}s, In-sample cross-Entropy error (Ein): {}, Binary error on the test set (Etest): {} \\n'.format(eta_0, t, round(end - start, 5), round(e_in, 5), round(test_error, 5)))\n",
    "\n",
    "# Other code here:"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimenting with iterations...\n",
      "\n",
      "Number of iterations: 10000, Training time : 15.06156s, In-sample Cross-Entropy error (Ein): 0.58471, Binary error on the training set (Etrain): 0.30921, Binary error on the test set (Etest): 0.31724 \n",
      "\n",
      "Number of iterations: 100000, Training time : 127.60057s, In-sample Cross-Entropy error (Ein): 0.4937, Binary error on the training set (Etrain): 0.22368, Binary error on the test set (Etest): 0.2069 \n",
      "\n",
      "Number of iterations: 1000000, Training time : 1251.69752s, In-sample Cross-Entropy error (Ein): 0.43535, Binary error on the training set (Etrain): 0.15132, Binary error on the test set (Etest): 0.13103 \n",
      "\n",
      "Experimenting with learning rates...\n",
      "\n",
      "Learning rate (η0): 0.01, Number of iterations: 23221, Training time : 28.42808s, In-sample cross-Entropy error (Ein): 0.40738, Binary error on the test set (Etest): 0.10345 \n",
      "\n",
      "Learning rate (η0): 0.1, Number of iterations: 2318, Training time : 2.82869s, In-sample cross-Entropy error (Ein): 0.40738, Binary error on the test set (Etest): 0.10345 \n",
      "\n",
      "Learning rate (η0): 1, Number of iterations: 228, Training time : 0.28176s, In-sample cross-Entropy error (Ein): 0.40738, Binary error on the test set (Etest): 0.10345 \n",
      "\n",
      "Learning rate (η0): 4, Number of iterations: 52, Training time : 0.06655s, In-sample cross-Entropy error (Ein): 0.40738, Binary error on the test set (Etest): 0.10345 \n",
      "\n",
      "Learning rate (η0): 5, Number of iterations: 40, Training time : 0.05286s, In-sample cross-Entropy error (Ein): 0.40738, Binary error on the test set (Etest): 0.10345 \n",
      "\n",
      "Learning rate (η0): 6, Number of iterations: 31, Training time : 0.03989s, In-sample cross-Entropy error (Ein): 0.40738, Binary error on the test set (Etest): 0.10345 \n",
      "\n",
      "Learning rate (η0): 7, Number of iterations: 45, Training time : 0.05788s, In-sample cross-Entropy error (Ein): 0.40738, Binary error on the test set (Etest): 0.10345 \n",
      "\n",
      "Learning rate (η0): 7.5, Number of iterations: 181, Training time : 0.22577s, In-sample cross-Entropy error (Ein): 0.40738, Binary error on the test set (Etest): 0.10345 \n",
      "\n",
      "Learning rate (η0): 7.6, Number of iterations: 452, Training time : 0.56235s, In-sample cross-Entropy error (Ein): 0.40738, Binary error on the test set (Etest): 0.10345 \n",
      "\n",
      "Learning rate (η0): 7.65, Number of iterations: 1767, Training time : 2.16061s, In-sample cross-Entropy error (Ein): 0.40738, Binary error on the test set (Etest): 0.10345 \n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z3mrOrbBLQuC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [],
   "execution_count": 6,
   "outputs": []
  }
 ]
}
